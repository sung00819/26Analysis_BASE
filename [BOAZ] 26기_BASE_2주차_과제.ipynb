{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTOK1lA7Hsz2"
      },
      "source": [
        "- 각자 이 ipynb 파일의 **사본을 생성**하여 과제 Q0~Q3까지 채운 후 해당 파일을 깃허브에 업로드해주세요!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2msLr-G13YUo"
      },
      "source": [
        "# RNN Sample Code in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JrL9Fl93d0T"
      },
      "source": [
        "- 직접 읽어보며 돌려볼 수 있는 **쉬운** 예제 코드~\n",
        "- 코드 간단 설명:\n",
        "  - 길이 12짜리 binary sequence(0/1)를 입력으로 받아서, 시퀀스 안에 1-0-1 pattern이 한 번이라도 등장하면 1, 아니면 0을 맞추는 binary classification을 수행하는 RNN 분류기\n",
        "  - 마지막에는 demo sequence로 예측 + hidden state 변화까지 출력하는 프로그램"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "EO04CkcR4bFR"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLXMSKiQ9k6l"
      },
      "source": [
        "- 정답 label 만드는 함수\n",
        "- 역할:\n",
        "  - sequence(e.g., [1,0,1,0,0,...]) 안에 연속된 3칸이 1-0-1인 구간이 있는지 검사\n",
        "  - 있으면 label=1 / 없으면 label=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "rfrge_Gq8ZFx"
      },
      "outputs": [],
      "source": [
        "def has_101_pattern(seq):\n",
        "    for i in range(len(seq) - 2):\n",
        "        if seq[i] == 1 and seq[i+1] == 0 and seq[i+2] == 1:\n",
        "            return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzaGLdAbGUSr"
      },
      "source": [
        "- 학습용 data 만드는 PatternDataset 클래스\n",
        "- 깨알 상식) PyTorch에서 Dataset은 \"데이터를 꺼내는 방식\"을 표준화한 클래스~\n",
        "  - 이걸 상속받아서 내가 하고자 하는 task에 부합하는 나만의 커스텀 Dataset 클래스를 만들어서 모델에 먹이는 겁니다 얍얍"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "4legZWrO8YJp"
      },
      "outputs": [],
      "source": [
        "# Dataset이 하는 일은 \"모델에 넣기 좋은 형태\"로 데이터를 제공하는 것!\n",
        "class PatternDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seq_len=12):\n",
        "        self.data = []\n",
        "        # (입력 시퀀스, 정답 label)을 n_samples개만큼 저장\n",
        "        for _ in range(n_samples):\n",
        "            seq = [random.randint(0, 1) for _ in range(seq_len)]  # 1) seq_len 길이의 random sequence 생성(0/1)\n",
        "            label = has_101_pattern(seq)                          # 2) has_101_pattern(seq)로 정답 label 생성\n",
        "            self.data.append((seq, label))                        # 3) (seq, label)을 self.data에 저장\n",
        "\n",
        "    # Dataset 안에 sample이 몇 개인지 알려주는 매직 메소드!\n",
        "    # 이걸로 보통 DataLoader가 \"전체 크기\"를 알 수 있게 합니다\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    # idx번째 데이터를 꺼내서 pytorch tensor로 변환해주는 매직 메소드\n",
        "    def __getitem__(self, idx):\n",
        "        seq, label = self.data[idx]\n",
        "        x = torch.tensor(seq, dtype=torch.long)         # (T,) = (12,) / embedding은 정수 인덱스를 받기 때문에 dtype으로 long을 사용합니다!\n",
        "        y = torch.tensor(label, dtype=torch.float32)    # scalar / BCEWithLogitsLoss가 float label(0.0/1.0)을 기대하는 편이라 dtype으로 float32를 사용했어요\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTROYjWUINLU"
      },
      "source": [
        "- RNN 모델 클래스\n",
        "- pytorch에서 모델 클래스는 일반적으로 nn.Module을 상속해서 만들어요~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "9B39dpKJ7Kl_"
      },
      "outputs": [],
      "source": [
        "class SimpleRNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=2, embed_dim=8, hidden_dim=16):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)  # 학습 연산을 위해 (0/1) -> '벡터'로 변환\n",
        "        self.rnn = nn.RNN(input_size=embed_dim, hidden_size=hidden_dim, batch_first=True) # sequence를 왼쪽부터 읽으면서 hidden state를 update\n",
        "        self.fc = nn.Linear(hidden_dim, 1) # 마지막 hidden state로 이진 분류 점수(logit) 출력\n",
        "\n",
        "    # 일반 학습/평가용 feedforward 순전파 함수\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        NOTE: B는 batch size, T는 시퀀스의 길이!\n",
        "\n",
        "        input x: (B, T) 0/1 token\n",
        "        return: logits (B,)\n",
        "        \"\"\"\n",
        "        emb = self.embed(x)            # (B, T, E) / 벡터화\n",
        "        out, h_n = self.rnn(emb)       # 각 시점의 hidden 기록인 out: (B, T, H) / 마지막 hidden state인 h_n: (1, B, H)\n",
        "        last_h = h_n[-1]               # (B, H) / 마지막 hidden만 추출\n",
        "        logits = self.fc(last_h)       # (B, H) -> (B, 1)\n",
        "        return logits.squeeze(1)       # (B,) / loss 계산 편하게 하기 위해 주로 이렇게 squeeze()라는 함수를 사용하여 모양을 맞춰줍니다\n",
        "\n",
        "    def forward_with_trace(self, x):\n",
        "        \"\"\"\n",
        "        시각화를 통해 이해할 수 있도록 time step별 hidden(out)과 마지막 예측(logits)을 함께 리턴하는 함수\n",
        "        x: (1, T) 단일 시퀀스만 넣는 것을 권장함\n",
        "        \"\"\"\n",
        "        emb = self.embed(x)            # (1, T, E)\n",
        "        out, h_n = self.rnn(emb)       # out: (1, T, H)\n",
        "        last_h = h_n[-1]               # (1, H)\n",
        "        logits = self.fc(last_h)       # (1, 1)\n",
        "        return logits.squeeze(1), out.squeeze(0)  # logits: (1,), out: (T, H)\n",
        "        # out.squeeze(0) 추가로 한 이유 : batch=1을 넣으면 shape이 (1,T,H)인데 이 batch의 차원(1)을 제거하여 (T,H)로 보기 좋게 만든것!\n",
        "        # (크게 중요한 건 아닌데 그냥 궁금하실까봐,,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXV7srCf9Jhm"
      },
      "source": [
        "- 메인 함수: train()\n",
        "- 내부 로직 STEP BY STEP 설명:\n",
        "  1. (train/val) dataset 생성\n",
        "  2. DataLoader로 배치 묶기\n",
        "  3. model / loss function / optimizer 준비\n",
        "  4. epoch 반복하며 train\n",
        "  5. epoch마다 검증(val) 정확도 출력\n",
        "  6. 마지막에 demo sequence 1개 넣어서 확률 출력\n",
        "  7. demo sequence에서 시간별 hidden state 일부 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAHlsjUj8Ntd",
        "outputId": "b2c72cfc-21e8-434f-a07f-6b4144e53206"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] train_loss=0.6012  val_acc=0.7250\n",
            "[Epoch 2] train_loss=0.5751  val_acc=0.7250\n",
            "[Epoch 3] train_loss=0.5202  val_acc=0.7810\n",
            "[Epoch 4] train_loss=0.4477  val_acc=0.8170\n",
            "[Epoch 5] train_loss=0.2632  val_acc=0.9510\n",
            "\n",
            "=== Demo ===\n",
            "Sequence: [1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0]\n",
            "Final prob(pattern=101): 0.3995\n",
            "\n",
            "Hidden state trace (show first 3 and last 3 time steps):\n",
            "t= 0, x_t=1 -> h_t[:6]=[-0.656, -0.213, -0.257, 0.401, 0.319, 0.845]\n",
            "t= 1, x_t=1 -> h_t[:6]=[-0.845, -0.593, -0.475, 0.552, 0.322, 0.977]\n",
            "t= 2, x_t=0 -> h_t[:6]=[0.959, 0.923, -0.449, -0.847, 0.637, 0.461]\n",
            "t= 9, x_t=1 -> h_t[:6]=[-0.908, -0.644, -0.737, 0.356, 0.589, 0.986]\n",
            "t=10, x_t=0 -> h_t[:6]=[0.967, 0.922, -0.79, -0.863, 0.807, 0.36]\n",
            "t=11, x_t=0 -> h_t[:6]=[0.998, 0.995, -0.332, -0.987, 0.477, -0.504]\n"
          ]
        }
      ],
      "source": [
        "def train():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    \"\"\"\n",
        "    <헷갈리는 개념 (코드 지피티 딸깍하지 말고 이젠 꼭 알아두자)>\n",
        "    - Dataset: 데이터 1개를 어떻게 꺼낼지 정의\n",
        "    - DataLoader: 여러 개를 묶어서 batch를 만들고, 섞고, 반복 가능한 형태로 제공\n",
        "    - shuffle : train은 섞어서 학습이 안정적이고 (True) / val은 평가하는 거니까 섞을 필요 없음 (False)\n",
        "    \"\"\"\n",
        "\n",
        "    train_ds = PatternDataset(n_samples=6000, seq_len=12)\n",
        "    val_ds   = PatternDataset(n_samples=1000, seq_len=12)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    # model/loss function/optimizer 준비\n",
        "    model = SimpleRNNClassifier(embed_dim=8, hidden_dim=16).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    # Epoch train loop\n",
        "    for epoch in range(1, 6):\n",
        "        model.train()                                   # dropout/batch regularization 등의 mode들이 train 모드로 바뀜\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x, y in train_loader:                       # batch 단위로 (x, y) 받음\n",
        "            x, y = x.to(device), y.to(device)           # x:(B,T), y:(B,)\n",
        "\n",
        "            logits = model(x)                           # (B,) / forward 실행\n",
        "            loss = criterion(logits, y)                 # scalar값 (정답 y와 예측 점수인 logit을 비교하여 loss값 계산)\n",
        "\n",
        "            optimizer.zero_grad()                       # 이전 gradient를 0으로\n",
        "            loss.backward()                             # backpropagation\n",
        "            optimizer.step()                            # parameter update\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "        train_loss = total_loss / len(train_loader.dataset)\n",
        "\n",
        "        # validation\n",
        "        model.eval()                                    # 평가 모드\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():                           # 평가이므로 학습 때와 달리 gradient 계산 하지 않음\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                logits = model(x)\n",
        "                probs = torch.sigmoid(logits)\n",
        "                preds = (probs >= 0.5).float()          # 0.5 이상이면 1로 예측하도록 구현\n",
        "                correct += (preds == y).sum().item()\n",
        "                total += y.numel()\n",
        "\n",
        "        val_acc = correct / total\n",
        "        print(f\"[Epoch {epoch}] train_loss={train_loss:.4f}  val_acc={val_acc:.4f}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # DEMO: hidden state 흐름을 출력해보자!\n",
        "    # ----------------------------\n",
        "    model.eval()\n",
        "\n",
        "    demo_seq = [1,1,0,0,1,1,0,0,1,1,0,0]  # 패턴 101이 있는 입력 시퀀스\n",
        "    demo = torch.tensor([demo_seq], dtype=torch.long).to(device)  # (1,T)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logit, h_trace = model.forward_with_trace(demo)\n",
        "        prob = torch.sigmoid(logit).item()\n",
        "\n",
        "    print(\"\\n=== Demo ===\")\n",
        "    print(\"Sequence:\", demo_seq)\n",
        "    print(\"Final prob(pattern=101):\", round(prob, 4))\n",
        "\n",
        "    # hidden trace 일부 출력(앞 3스텝 + 마지막 3스텝)\n",
        "    h_cpu = h_trace.cpu()  # (T,H)\n",
        "    print(\"\\nHidden state trace (show first 3 and last 3 time steps):\")\n",
        "    for t in list(range(3)) + list(range(len(demo_seq)-3, len(demo_seq))):\n",
        "        vec = h_cpu[t][:6].tolist()  # hidden_dim 16 중 앞 6개만 보기\n",
        "        vec = [round(v, 3) for v in vec]\n",
        "        print(f\"t={t:2d}, x_t={demo_seq[t]} -> h_t[:6]={vec}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkUeto3kjsbW"
      },
      "source": [
        "- 위 코드에서 demo_seq 변수를 아래 두 가지로 바꿔서 각각 실행해보세요~\n",
        "  - 패턴 있음: [1,0,1,0,0,0,0,0,0,0,0,0] → 확률 높아야 함\n",
        "  - 패턴 없음: [1,1,0,0,1,1,0,0,1,1,0,0] → 확률 낮아야 함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBSysnq3kNjm"
      },
      "source": [
        "### Q0. 위 코드의 출력 결과 분석 & 두 가지 입력을 넣었을 때 각각의 결과를 비교 분석하시오."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pifQ80I8kZr_"
      },
      "source": [
        "Ans) \n",
        "위 코드는 에폭5까지의 loss와 accuracy를 보여주는데, 에폭이 진행될수록 loss는 줄고 accuracy는 오르는 것을 볼 수 있어 모델이 패턴을 찾는 규칙을 파악했다고 볼 수 있다.\n",
        "hidden state는 시간의 진행과 그때의 x값을 보여주며 패턴을 학습하는 모습을 보여준다.\n",
        "패턴이 있는 [1,0,1,0,0,0,0,0,0,0,0,0]은 0.9666, 패턴이 없는 [1,1,0,0,1,1,0,0,1,1,0,0]은 0.3995로 각각 0.5보다 크거나 작게 알맞은 결과가 나왔다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njt9mJdlkiDQ"
      },
      "source": [
        "# LSTM Sample code in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8lrNsxJFRGy"
      },
      "source": [
        "- 아래는 위와 동일하게 진행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "kW-QeXmXkhF9"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "XMoSaivQFF7Q"
      },
      "outputs": [],
      "source": [
        "def has_101_pattern(seq):\n",
        "    for i in range(len(seq) - 2):\n",
        "        if seq[i] == 1 and seq[i+1] == 0 and seq[i+2] == 1:\n",
        "            return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "uo-gXIZOFF5Q"
      },
      "outputs": [],
      "source": [
        "class PatternDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seq_len=12):\n",
        "        self.data = []\n",
        "        for _ in range(n_samples):\n",
        "            seq = [random.randint(0, 1) for _ in range(seq_len)]\n",
        "            label = has_101_pattern(seq)\n",
        "            self.data.append((seq, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq, label = self.data[idx]\n",
        "        x = torch.tensor(seq, dtype=torch.long)\n",
        "        y = torch.tensor([label], dtype=torch.float)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hzsnnU5FL2k"
      },
      "source": [
        "- 여기서부터 LSTM 모델 클래스"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "FyAeyB5sFF2b"
      },
      "outputs": [],
      "source": [
        "class SimpleLSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=2, embed_dim=8, hidden_dim=16):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)  # (0/1) -> 벡터로 변환\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embed(x)                         # (batch, seq_len, embed_dim)\n",
        "        out, (h_n, c_n) = self.lstm(emb)            # out: (batch, seq_len, hidden_dim)\n",
        "                                                    # h_n: (num_layers, batch, hidden_dim)\n",
        "                                                    # c_n: (num_layers, batch, hidden_dim)\n",
        "\n",
        "        last_h = h_n[-1]                            # (batch, hidden_dim)  마지막 layer의 마지막 hidden\n",
        "        logit = self.fc(last_h)                     # (batch, 1)\n",
        "        return logit, out, (h_n, c_n)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAsdRO6FFUe-"
      },
      "source": [
        "RNN과의 차이점??\n",
        "\n",
        "1. nn.RNN -> nn.LSTM\n",
        "2. lSTM은 hidden state(h) 말고도 cell state(c)가 추가되었다는 점\n",
        "3. forward 결과에 따른 형태? (output, (h_n, c_n))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5EtTq-8FVOX"
      },
      "source": [
        "- 메인 함수 : train()\n",
        "- 내부 로직 step by step 설명:\n",
        "  1. train, val dataset 생성\n",
        "  2. DataLoader로 배치 묶기\n",
        "  3. model, loss function, optimizer 준비\n",
        "  4. epoch 반복하며 train\n",
        "  5. epoch마다 검증 정확도 출력\n",
        "  6. 마지막에 demo seq 하나 넣어서 확률 출력\n",
        "  7. demo seq에서 시간별 hidden state 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "stLhRCNMFF0L"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    train_ds = PatternDataset(n_samples=6000, seq_len=12)\n",
        "    val_ds   = PatternDataset(n_samples=1000, seq_len=12)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    model = SimpleLSTMClassifier(vocab_size=2, embed_dim=8, hidden_dim=16).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()  # logit을 바로 넣는 BCE\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    n_epochs = 5\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        # ---- train ----\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)               # x:(B,12), y:(B,1)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logit, _, _ = model(x)                           # logit:(B,1)\n",
        "            loss = criterion(logit, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(train_ds)\n",
        "\n",
        "        # ---- val ----\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                logit, _, _ = model(x)\n",
        "                prob = torch.sigmoid(logit)                  # (B,1)\n",
        "                pred = (prob >= 0.5).float()                 # (B,1)\n",
        "                correct += (pred == y).sum().item()\n",
        "                total += y.numel()\n",
        "\n",
        "        acc = correct / total\n",
        "        print(f\"[Epoch {epoch:02d}] loss={avg_loss:.4f} | val_acc={acc:.4f}\")\n",
        "\n",
        "    # ---- demo ----\n",
        "    model.eval()\n",
        "\n",
        "    demo_seq = [1,0,1,0,0,0,1,1,0,0,0,0] # 패턴 없음 -> 확률 낮아야 함\n",
        "\n",
        "    x_demo = torch.tensor(demo_seq, dtype=torch.long).unsqueeze(0).to(device)  # (1, 12)\n",
        "    logit, out_all, (h_n, c_n) = model(x_demo)\n",
        "\n",
        "    prob = torch.sigmoid(logit).item()\n",
        "    print(\"\\n--- DEMO ---\")\n",
        "    print(\"demo_seq:\", demo_seq)\n",
        "    print(f\"pred_prob(pattern=1): {prob:.4f}\")\n",
        "\n",
        "    # 시간별 hidden state 일부 출력\n",
        "    # out_all: (1, seq_len, hidden_dim)  -> time step별 hidden이 들어있음 (LSTM의 output)\n",
        "    out_all = out_all.squeeze(0).detach().cpu()  # (seq_len, hidden_dim)\n",
        "\n",
        "    print(\"\\n[time step별 hidden state 앞 6개 차원만 출력]\")\n",
        "    for t in range(out_all.size(0)):\n",
        "        h_t = out_all[t, :6].numpy()\n",
        "        print(f\"t={t:02d}, x={demo_seq[t]} -> h_t[:6]={h_t}\")\n",
        "\n",
        "    # (참고) 마지막 hidden/cell state도 같이 보기\n",
        "    last_h = h_n[-1].squeeze(0).detach().cpu()    # (hidden_dim,)\n",
        "    last_c = c_n[-1].squeeze(0).detach().cpu()    # (hidden_dim,)\n",
        "    print(\"\\n[마지막 state 요약]\")\n",
        "    print(\"last_h[:6] =\", last_h[:6].numpy())\n",
        "    print(\"last_c[:6] =\", last_c[:6].numpy())\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QxJeYuWFcOQ"
      },
      "source": [
        "돌려돌려"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSe_IZ-DFFyD",
        "outputId": "136e023e-5808-463e-f1c8-0918d0538d15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 01] loss=0.6053 | val_acc=0.7520\n",
            "[Epoch 02] loss=0.5231 | val_acc=0.7790\n",
            "[Epoch 03] loss=0.4921 | val_acc=0.7830\n",
            "[Epoch 04] loss=0.3841 | val_acc=0.8850\n",
            "[Epoch 05] loss=0.2016 | val_acc=0.9930\n",
            "\n",
            "--- DEMO ---\n",
            "demo_seq: [1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0]\n",
            "pred_prob(pattern=1): 0.9825\n",
            "\n",
            "[time step별 hidden state 앞 6개 차원만 출력]\n",
            "t=00, x=1 -> h_t[:6]=[-0.14174482 -0.08512197  0.08501675  0.0131529   0.31470555 -0.37746838]\n",
            "t=01, x=0 -> h_t[:6]=[-0.19705339 -0.61866325  0.6823895  -0.22395273  0.09037465 -0.14640789]\n",
            "t=02, x=1 -> h_t[:6]=[-0.48691863 -0.03071117  0.05138782 -0.41100624  0.7116504  -0.72449905]\n",
            "t=03, x=0 -> h_t[:6]=[-0.6413352  -0.3942228   0.8106143  -0.58283067  0.62902397 -0.6043566 ]\n",
            "t=04, x=0 -> h_t[:6]=[-0.33412886 -0.00416821  0.59450585 -0.7977264   0.7439764  -0.5170861 ]\n",
            "t=05, x=0 -> h_t[:6]=[-0.25388196  0.01199484  0.65564144 -0.86128384  0.8482886  -0.48781684]\n",
            "t=06, x=1 -> h_t[:6]=[-0.65437245 -0.02250233  0.12161121 -0.955305    0.9512752  -0.793125  ]\n",
            "t=07, x=1 -> h_t[:6]=[-0.8641172  -0.02263333  0.18842061 -0.97463465  0.9715485  -0.88884205]\n",
            "t=08, x=0 -> h_t[:6]=[-0.89275265  0.0369685   0.79015857 -0.94035524  0.9617375  -0.9039846 ]\n",
            "t=09, x=0 -> h_t[:6]=[-0.8718254   0.05293984  0.7217512  -0.9497226   0.97786653 -0.9216342 ]\n",
            "t=10, x=0 -> h_t[:6]=[-0.8644581   0.05412147  0.7263567  -0.95106393  0.9790379  -0.93066484]\n",
            "t=11, x=0 -> h_t[:6]=[-0.86531895  0.05801378  0.7265573  -0.9502241   0.9788835  -0.93802273]\n",
            "\n",
            "[마지막 state 요약]\n",
            "last_h[:6] = [-0.86531895  0.05801378  0.7265573  -0.9502241   0.9788835  -0.93802273]\n",
            "last_c[:6] = [-1.3367627   0.16688694  1.6894195  -4.1327076   4.197921   -1.9294138 ]\n"
          ]
        }
      ],
      "source": [
        "model = train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XZsF577Fgy1"
      },
      "source": [
        "- RNN과 비교할 점 :\n",
        "  1. val_acc\n",
        "  2. train_loss\n",
        "  3. demo prob\n",
        "\n",
        "\n",
        "- RNN vs LSTM\n",
        "\n",
        "  현재는 장기기억이 필요 없어서 유사한 상황.\n",
        "  \n",
        "  trade-off 중요성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRjTLGz8FkJx"
      },
      "source": [
        "# GRU Sample code in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "FZQRzd_yFFwH"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "IyxVJD_HFFtX"
      },
      "outputs": [],
      "source": [
        "def has_101_pattern(seq):\n",
        "    for i in range(len(seq) - 2):\n",
        "        if seq[i] == 1 and seq[i+1] == 0 and seq[i+2] == 1:\n",
        "            return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "Py8J1mxyFFq3"
      },
      "outputs": [],
      "source": [
        "class PatternDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seq_len=12):\n",
        "        self.data = []\n",
        "        for _ in range(n_samples):\n",
        "            seq = [random.randint(0, 1) for _ in range(seq_len)]\n",
        "            label = has_101_pattern(seq)\n",
        "            self.data.append((seq, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq, label = self.data[idx]\n",
        "        x = torch.tensor(seq, dtype=torch.long)\n",
        "        y = torch.tensor([label], dtype=torch.float)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXUAnZxhFtM_"
      },
      "source": [
        "- GRU 클래스:\n",
        "  nn.GRU\n",
        "  \n",
        "  LSTM처럼 cell state(c)가 없고, hidden state(h) 하나만 유지\n",
        "\n",
        "  forward 결과로 out, h_n\n",
        "  \n",
        "  out : 모든 time step의 hidden (batch, seq_len, hidden_dim)\n",
        "\n",
        "  h_n : 마지막 hidden (num_layers, batch, hidden_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "n_7qciOPFFoo"
      },
      "outputs": [],
      "source": [
        "class SimpleGRUClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=2, embed_dim=8, hidden_dim=16):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embed(x)                 # (B, T, E)\n",
        "        out, h_n = self.gru(emb)            # out: (B, T, H), h_n: (1, B, H)\n",
        "        last_h = h_n[-1]                    # (B, H)\n",
        "        logit = self.fc(last_h)             # (B, 1)\n",
        "        return logit, out, h_n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "nrZXy-8iFFlr"
      },
      "outputs": [],
      "source": [
        "def train_gru():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    train_ds = PatternDataset(n_samples=6000, seq_len=12)\n",
        "    val_ds   = PatternDataset(n_samples=1000, seq_len=12)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    model = SimpleGRUClassifier(vocab_size=2, embed_dim=8, hidden_dim=16).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    n_epochs = 5\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        # ---- train ----\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logit, _, _ = model(x)\n",
        "            loss = criterion(logit, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(train_ds)\n",
        "\n",
        "        # ---- val ----\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                logit, _, _ = model(x)\n",
        "                prob = torch.sigmoid(logit)\n",
        "                pred = (prob >= 0.5).float()\n",
        "                correct += (pred == y).sum().item()\n",
        "                total += y.numel()\n",
        "\n",
        "        acc = correct / total\n",
        "        print(f\"[Epoch {epoch}] train_loss={avg_loss:.4f}  val_acc={acc:.4f}\")\n",
        "\n",
        "    # ---- demo ----\n",
        "    model.eval()\n",
        "\n",
        "    demo_seq = [1,0,1,0,0,0,1,1,0,0,0,0]  # 패턴 있음\n",
        "    x_demo = torch.tensor(demo_seq, dtype=torch.long).unsqueeze(0).to(device)  # (1, 12)\n",
        "\n",
        "    logit, out_all, h_n = model(x_demo)\n",
        "    prob = torch.sigmoid(logit).item()\n",
        "\n",
        "    print(\"\\n=== Demo ===\")\n",
        "    print(\"Sequence:\", demo_seq)\n",
        "    print(f\"Final prob(pattern=101): {prob:.4f}\")\n",
        "\n",
        "    # time step별 hidden state 출력 (앞 6개 차원)\n",
        "    out_all = out_all.squeeze(0).detach().cpu()  # (T, H)\n",
        "\n",
        "    print(\"\\nHidden state trace (show first 3 and last 3 time steps):\")\n",
        "    T = out_all.size(0)\n",
        "    for t in list(range(3)) + list(range(T-3, T)):\n",
        "        h_t = out_all[t, :6].numpy()\n",
        "        # 보기 좋게 소수점 3자리로\n",
        "        h_t_fmt = [float(f\"{v:.3f}\") for v in h_t]\n",
        "        print(f\"t={t:2d}, x_t={demo_seq[t]} -> h_t[:6]={h_t_fmt}\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARHGR8JkFFjY",
        "outputId": "174a041d-9d4a-49f0-bd08-686799da70c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] train_loss=0.6305  val_acc=0.7640\n",
            "[Epoch 2] train_loss=0.5555  val_acc=0.7670\n",
            "[Epoch 3] train_loss=0.5150  val_acc=0.7830\n",
            "[Epoch 4] train_loss=0.4613  val_acc=0.8280\n",
            "[Epoch 5] train_loss=0.2547  val_acc=0.9960\n",
            "\n",
            "=== Demo ===\n",
            "Sequence: [1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0]\n",
            "Final prob(pattern=101): 0.9818\n",
            "\n",
            "Hidden state trace (show first 3 and last 3 time steps):\n",
            "t= 0, x_t=1 -> h_t[:6]=[0.555, 0.392, -0.015, -0.27, -0.441, 0.004]\n",
            "t= 1, x_t=0 -> h_t[:6]=[0.038, -0.735, 0.008, 0.24, -0.387, -0.219]\n",
            "t= 2, x_t=1 -> h_t[:6]=[0.794, -0.588, 0.734, -0.753, -0.832, -0.196]\n",
            "t= 9, x_t=0 -> h_t[:6]=[0.9, -0.927, 0.977, -0.938, -0.898, -0.939]\n",
            "t=10, x_t=0 -> h_t[:6]=[0.895, -0.947, 0.977, -0.934, -0.892, -0.936]\n",
            "t=11, x_t=0 -> h_t[:6]=[0.893, -0.958, 0.976, -0.932, -0.888, -0.934]\n"
          ]
        }
      ],
      "source": [
        "gru_model = train_gru()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnW68WglF24v"
      },
      "source": [
        "- LSTM vs GRU ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmuBRGxgF5ne"
      },
      "source": [
        "# LSTM & GRU 과제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM42wRXOF8Tu"
      },
      "source": [
        "모델의 장기기억 성능을 비교하기 위한 코드입니다.\n",
        "\n",
        "코드 중간의 빈칸을 채우면서, 매애앤 아래의 답변을 채워주시면 됩니다.\n",
        "\n",
        "모르면 인공지능을 사용해도 좋지만, sample code로도 풀 수 있으니 최대한 본인의 힘으로 해보면 좋겠습니다 !!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "bItDmp57FyrU"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QkVj3qZuFyqA",
        "outputId": "7189742c-9391-4e71-9a69-bfa5b076579e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tmi) 11/7은 제 생일입니다. 감사합니다.\n",
        "def set_seed(seed=117):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(117)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "hZrbG1IeFyoJ"
      },
      "outputs": [],
      "source": [
        "class LongMemoryDataset(Dataset):\n",
        "\n",
        "    def __init__(self, n_samples=8000, T=80):\n",
        "        self.T = T\n",
        "        self.data = []\n",
        "        for _ in range(n_samples):\n",
        "            first_bit = random.randint(0, 1)          # 기억해야 할 정보\n",
        "            middle = [random.randint(0, 1) for _ in range(T-2)]\n",
        "            seq = [first_bit] + middle + [2]          # 마지막은 DELIM=2\n",
        "            label = first_bit\n",
        "            self.data.append((seq, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq, label = self.data[idx]\n",
        "        x = torch.tensor(seq, dtype=torch.long)               # (T,)\n",
        "        y = torch.tensor([label], dtype=torch.float)          # (1,)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLi1JgS9GFQ8"
      },
      "source": [
        "1. lstm 모델 빈칸 채우기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "HWz4-GNaFymh"
      },
      "outputs": [],
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=3, embed_dim=8, hidden_dim=32):\n",
        "        super().__init__()\n",
        "        # TODO : 임베딩 레이어를 선언하세요.\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        # TODO : LSTM 레이어를 선언하세요. (batch_first=True)\n",
        "        self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim,batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T)\n",
        "        # TODO: 임베딩을 통과시키세요.\n",
        "        emb = self.embed(x)\n",
        "        # TODO : LSTM에 넣고 out, (h_n, c_n)을 받으세요.\n",
        "        out, (h_n, c_n) = self.lstm(emb)\n",
        "        # TODO : 마지막 hidden(last_h)을 얻으세요.\n",
        "        last_h = h_n[-1]\n",
        "        logit = self.fc(last_h)\n",
        "        return logit, out, (h_n, c_n)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3dXxnYlGGih"
      },
      "source": [
        "2. gru 모델 빈칸 채우기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "SXYh1847Fyko"
      },
      "outputs": [],
      "source": [
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=3, embed_dim=8, hidden_dim=32):\n",
        "        super().__init__()\n",
        "        # TODO : 임베딩 레이어\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        # TODO : GRU 레이어 (batch_first=True)\n",
        "        self.gru = nn.GRU(input_size=embed_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T)\n",
        "        # TODO : 임베딩\n",
        "        emb = self.embed(x)\n",
        "        # TODO : GRU forward로 out, h_n 받기\n",
        "        out, h_n = self.gru(emb)\n",
        "        # TODO : 마지막 hidden\n",
        "        last_h = h_n[-1]\n",
        "        logit = self.fc(last_h)\n",
        "        return logit, out, h_n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogOE47LuGP7i"
      },
      "source": [
        "학습 루프 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "ELMItPqMGRG2"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, epochs=6, lr=1e-3, device=\"cpu\", tag=\"\"):\n",
        "    model = model.to(device)\n",
        "    # TODO : loss 함수 선언 (BCEWithLogitsLoss)\n",
        "    crit = nn.BCEWithLogitsLoss()\n",
        "    # TODO : optimizer 선언 (Adam)\n",
        "    opt = torch.optim.Adam(model.parameters(),lr=lr)\n",
        "\n",
        "    #추가:그래프를 위해 loss history만들기\n",
        "    history = {\"train_loss\": [], \"val_acc\": []}\n",
        "    \n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        n = 0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # TODO : gradient 초기화\n",
        "            opt.zero_grad()\n",
        "            out = model(x)\n",
        "            logit = out[0] if isinstance(out, (tuple, list)) else out\n",
        "\n",
        "            # TODO : loss 계산\n",
        "            loss = crit(logit,y.float())\n",
        "            # TODO : backprop\n",
        "            loss.backward()\n",
        "            # TODO : optimizer step\n",
        "            opt.step()\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            n += x.size(0)\n",
        "\n",
        "        train_loss = total_loss / n\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                out = model(x)\n",
        "                logit = out[0] if isinstance(out, (tuple, list)) else out\n",
        "\n",
        "                # TODO : prob = sigmoid(logit)\n",
        "                prob = torch.sigmoid(logit)\n",
        "                # TODO : pred = (prob >= 0.5)\n",
        "                pred = (prob >= 0.5).float() \n",
        "\n",
        "\n",
        "                correct += (pred == y).sum().item()\n",
        "                total += y.numel()\n",
        "\n",
        "        val_acc = correct / total\n",
        "        print(f\"{tag}[Epoch {ep}] train_loss={train_loss:.4f}  val_acc={val_acc:.4f}\")\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "MqgsERicGRFS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== LSTM ===\n",
            "LSTM [Epoch 1] train_loss=0.6937  val_acc=0.4860\n",
            "LSTM [Epoch 2] train_loss=0.6934  val_acc=0.4860\n",
            "LSTM [Epoch 3] train_loss=0.6934  val_acc=0.4860\n",
            "LSTM [Epoch 4] train_loss=0.6934  val_acc=0.4840\n",
            "LSTM [Epoch 5] train_loss=0.6935  val_acc=0.5180\n",
            "LSTM [Epoch 6] train_loss=0.6933  val_acc=0.4840\n",
            "\n",
            "=== GRU ===\n",
            "GRU  [Epoch 1] train_loss=0.6946  val_acc=0.4825\n",
            "GRU  [Epoch 2] train_loss=0.6935  val_acc=0.5140\n",
            "GRU  [Epoch 3] train_loss=0.6933  val_acc=0.4845\n",
            "GRU  [Epoch 4] train_loss=0.6935  val_acc=0.4860\n",
            "GRU  [Epoch 5] train_loss=0.6933  val_acc=0.5140\n",
            "GRU  [Epoch 6] train_loss=0.6933  val_acc=0.5140\n"
          ]
        }
      ],
      "source": [
        "# 그대로 실행하시면 됩니다.\n",
        "\n",
        "T = 80\n",
        "train_ds = LongMemoryDataset(n_samples=8000, T=T)\n",
        "val_ds   = LongMemoryDataset(n_samples=2000, T=T)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "lstm = LSTMClassifier(vocab_size=3, embed_dim=8, hidden_dim=32)\n",
        "gru  = GRUClassifier(vocab_size=3, embed_dim=8, hidden_dim=32)\n",
        "\n",
        "print(\"=== LSTM ===\")\n",
        "lstm = train_model(lstm, train_loader, val_loader, epochs=6, lr=1e-3, device=device, tag=\"LSTM \")\n",
        "\n",
        "print(\"\\n=== GRU ===\")\n",
        "gru = train_model(gru, train_loader, val_loader, epochs=6, lr=1e-3, device=device, tag=\"GRU  \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8o9PlFmGZMp"
      },
      "source": [
        "### Q1. LSTM과 GRU의 차이점에 대해서 간략하게 서술해주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfE8A2EFGeHV"
      },
      "source": [
        "Ans) GRU는 hidden state로만 과거 정보를 기억해 구조가 단순하고 연산 효율이 좋은 반면, LSTM는 cell state를 따로 두어 기울기 소실 문제를 해결했다. 계산량이 조금 많지만 복잡한 장기기억에 유리하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRfrs3zGGiDa"
      },
      "source": [
        "### Q2. T=80에서 LSTM과 GRU의 학습 곡선을 비교하고, 어느 쪽이 더 안정적으로 수렴했는지 서술해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LSTM [Epoch 1] train_loss=0.6935  val_acc=0.5140\n",
            "LSTM [Epoch 2] train_loss=0.6933  val_acc=0.4860\n",
            "LSTM [Epoch 3] train_loss=0.6933  val_acc=0.4860\n",
            "LSTM [Epoch 4] train_loss=0.6932  val_acc=0.4850\n",
            "LSTM [Epoch 5] train_loss=0.6932  val_acc=0.5140\n",
            "LSTM [Epoch 6] train_loss=0.6933  val_acc=0.5195\n",
            "GRU  [Epoch 1] train_loss=0.6938  val_acc=0.5140\n",
            "GRU  [Epoch 2] train_loss=0.6933  val_acc=0.5140\n",
            "GRU  [Epoch 3] train_loss=0.6934  val_acc=0.4890\n",
            "GRU  [Epoch 4] train_loss=0.6934  val_acc=0.4860\n",
            "GRU  [Epoch 5] train_loss=0.6933  val_acc=0.4915\n",
            "GRU  [Epoch 6] train_loss=0.6932  val_acc=0.5105\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "lstm = LSTMClassifier()\n",
        "lstm, h_lstm = train_model(lstm, train_loader, val_loader, epochs=6, lr=1e-3, device=device, tag=\"LSTM \")\n",
        "\n",
        "gru = GRUClassifier()\n",
        "gru, h_gru = train_model(gru, train_loader, val_loader, epochs=6, lr=1e-3, device=device, tag=\"GRU  \")\n",
        "\n",
        "# train loss curve\n",
        "plt.figure()\n",
        "plt.plot(h_lstm[\"train_loss\"], label=\"LSTM train_loss\")\n",
        "plt.plot(h_gru[\"train_loss\"], label=\"GRU train_loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.legend()\n",
        "plt.ylim(0.67,0.72)\n",
        "plt.show()\n",
        "\n",
        "# val acc curve\n",
        "plt.figure()\n",
        "plt.plot(h_lstm[\"val_acc\"], label=\"LSTM val_acc\")\n",
        "plt.plot(h_gru[\"val_acc\"], label=\"GRU val_acc\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.legend()\n",
        "plt.ylim(0.3,0.8)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjAVyZcEGiBq"
      },
      "source": [
        "Ans) T=80은 시퀀스가 짧기에 cell state를 따로 둘 필요가 없는 GRU가 성능이 더 좋기를 기대했으나 막상 비슷한 결과가 나왔다. 아마 데이터가 적고 구조가 쉬웠기 때문에 비슷한 정확도와 loss가 나온 것 같다. 대신 내부적으로는 GRU가 더 간단한 구조를 가지고 파라미터 수도 저기 때문에 학습속도가 더 빨랐을 것 같다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWhHz27cGiAD"
      },
      "source": [
        "### Q3. T를 80 → 150 → 300 순으로 늘려서 각각 실행해보고, 어떤 모델이 성능을 더 잘 유지하는지, 왜 그런 것 같은지를 서술해주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpLgdVqOGh66"
      },
      "source": [
        "Ans) LSTM이 성능을 더 잘 유지한다. 현재 출력되는 연산공간인 hidden state와는 별도로 cell state를 두어 오래된 시점의 데이터에 대한 기울기 소실 문제를 해결했기 때문에 정보를 더 오래 기억할 수 있고, 따라서 긴 시퀀스의 연산에서 장점이 두드러진다. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
